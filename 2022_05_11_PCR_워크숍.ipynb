{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2022-05-11 PCR 워크숍.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/saeone/LEMS_PCR/blob/main/2022_05_11_PCR_%EC%9B%8C%ED%81%AC%EC%88%8D.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2022-05-11 PCR 워크숍\n",
        "한국어 자연어처리 실습 (1)\n",
        "\n",
        "sources:\n",
        "- https://wikidocs.net/book/2155\n"
      ],
      "metadata": {
        "id": "8GddkPhAXIy6"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install soynlp"
      ],
      "metadata": {
        "id": "6gjZjBxmHE1L"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install konlpy"
      ],
      "metadata": {
        "id": "Jtfcl22DswKM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install https://github.com/pandas-profiling/pandas-profiling/archive/master.zip"
      ],
      "metadata": {
        "id": "hAhdPtlCZC-j"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "import string, re\n",
        "\n",
        "from pandas_profiling import ProfileReport\n",
        "\n",
        "from soynlp.word import WordExtractor\n",
        "from soynlp.noun import LRNounExtractor_v2\n",
        "from soynlp.tokenizer import LTokenizer, RegexTokenizer\n",
        "\n",
        "from sklearn.linear_model import LogisticRegression, Ridge\n",
        "from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn import metrics\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, classification_report\n",
        "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "%matplotlib inline "
      ],
      "metadata": {
        "id": "Gs5zZES35w16"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def overview_df(df):\n",
        "    \"\"\"Take a dataframe and print an overview of the dataframe\n",
        "       For each column: \n",
        "       - Identify the Data Types - Numpy\n",
        "       - Count the unique values\n",
        "       - Count missing values\n",
        "       - Count for each variable \n",
        "       - Count of zero values\n",
        "    \"\"\"\n",
        "    if isinstance(df, pd.DataFrame):\n",
        "        data_dd = pd.DataFrame(df.dtypes,columns=['Numpy Dtype'])\n",
        "        data_dd['Nunique'] = df.nunique()\n",
        "        data_dd['MissingValues'] = df.isnull().sum()\n",
        "        data_dd['Count'] = df.count()\n",
        "        data_dd['ZeroValues'] = (df==0).sum()\n",
        "        # print(data_dd)\n",
        "        return data_dd\n",
        "    else:\n",
        "        print(\"Not a pandas dataframe\")"
      ],
      "metadata": {
        "id": "i5ZHnRYj5wwH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Read data / data 전처리 "
      ],
      "metadata": {
        "id": "xKQ6iObDX_ux"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "data = pd.read_pickle('https://sae.sgp1.digitaloceanspaces.com/20220511_PCR_Workshop/2022-05-11%20data.pickle?X-Amz-Algorithm=AWS4-HMAC-SHA256&X-Amz-Credential=5OETIOAHDDW42RUXRM23%2F20220510%2Fsgp1%2Fs3%2Faws4_request&X-Amz-Date=20220510T173900Z&X-Amz-Expires=604800&X-Amz-SignedHeaders=host&X-Amz-Signature=1dc80216acdd5cedc2068d0cb20fd1d5e28873ba45ca9e4c259852a9182e50d4')"
      ],
      "metadata": {
        "id": "Iop-EYLb5w44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.head()"
      ],
      "metadata": {
        "id": "xQmhGjL_5wzR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overview_df(data)"
      ],
      "metadata": {
        "id": "ZiHSAz1bYXyC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile1 = ProfileReport(data)"
      ],
      "metadata": {
        "id": "LHgJyfk9ZIJk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "profile1"
      ],
      "metadata": {
        "id": "IsRD0ee9avmZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data.columns"
      ],
      "metadata": {
        "id": "2v2XACTt5wZV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected = data[['성별', '나이', 'KTAS level', '혈압_c', '맥박수_c', '호흡수_c', '체온_c', '산소포화도_c', '주증상', '내원동기', '환자반응', '통증 부위', '통증강도_c', '질병여부', '과거질환 종류']]"
      ],
      "metadata": {
        "id": "Wnx1um5T_aSr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected.head()"
      ],
      "metadata": {
        "id": "-m24SytO_kBx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected['내원동기'] = data_selected['주증상'] + ' ' + data_selected['내원동기']"
      ],
      "metadata": {
        "id": "MCjt5_qDEH-S"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected.head()"
      ],
      "metadata": {
        "id": "Nc2o-mIgE9uR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected = data_selected.drop('주증상', axis=1)"
      ],
      "metadata": {
        "id": "SJsKr0tHFAP6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected['통증 부위'].value_counts()"
      ],
      "metadata": {
        "id": "RMm0ZCdK_k5a"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dummies = pd.get_dummies(data[\"성별\"], prefix='성별')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"혈압_c\"], prefix='혈압_c')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"맥박수_c\"], prefix='맥박수_c')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"호흡수_c\"], prefix='호흡수_c')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"체온_c\"], prefix='체온_c')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"산소포화도_c\"], prefix='산소포화도_c')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"환자반응\"], prefix='환자반응')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data['통증 부위'], prefix='통증부위')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data['통증강도_c'], prefix='통증강도_c')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"질병여부\"], prefix='질병여부')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "dummies = pd.get_dummies(data[\"과거질환 종류\"], prefix='과거질환종류')\n",
        "dummies.drop(dummies.columns[0], axis=1, inplace=True)\n",
        "data_selected = pd.concat([data_selected, dummies], axis=1)\n",
        "\n",
        "\n",
        "data_selected = data_selected.drop(['성별', \n",
        "                                    '혈압_c', \n",
        "                                    '맥박수_c',\n",
        "                                    '호흡수_c',\n",
        "                                    '체온_c',\n",
        "                                    '산소포화도_c',\n",
        "                                    '환자반응',\n",
        "                                    '통증 부위', \n",
        "                                    '통증강도_c', \n",
        "                                    '질병여부', \n",
        "                                    '과거질환 종류'], \n",
        "                                   axis=1)\n",
        "\n",
        "print(data_selected.shape)"
      ],
      "metadata": {
        "id": "HwqWY5yE_k22"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "data_selected.head()"
      ],
      "metadata": {
        "id": "pddtufzacXqI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "overview_df(data_selected)"
      ],
      "metadata": {
        "id": "wo6a2_2zdANO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Categorical data v Text data "
      ],
      "metadata": {
        "id": "JevTKlgsdOW0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_selected.drop(['KTAS level', '내원동기'], axis=1)\n",
        "y = data_selected['KTAS level']\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, stratify=y, random_state=1234)"
      ],
      "metadata": {
        "id": "T5lzynXnF3qz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train.head()"
      ],
      "metadata": {
        "id": "ct6MylHPF3lJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=50, random_state=1234, verbose=1, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "o-cOumTmGHMz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_selected['내원동기']\n",
        "y = data_selected['KTAS level']"
      ],
      "metadata": {
        "id": "lauZxJ_Bf8qh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text BoW only \n",
        "countvec = CountVectorizer()\n",
        "cdf = countvec.fit_transform(X)\n",
        "bow = pd.DataFrame(cdf.toarray(), columns = countvec.get_feature_names())\n",
        "bow\n"
      ],
      "metadata": {
        "id": "ojC5fvVPhcGq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countvec.get_feature_names()"
      ],
      "metadata": {
        "id": "TOKGu7ZLjj0I"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(bow, y, test_size=0.2, stratify=y, random_state=1234)\n",
        "X_train.head()"
      ],
      "metadata": {
        "id": "F9GVXymMgWVJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rf = RandomForestClassifier(n_estimators=50, random_state=1234, verbose=1, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "XSk7HdIegZNn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Text TFIDF \n",
        "X = data_selected['내원동기']\n",
        "tvec = TfidfVectorizer()\n",
        "tdf = tvec.fit_transform(X)\n",
        "tfidf = pd.DataFrame(tdf.toarray(), columns = tvec.get_feature_names())\n",
        "tfidf"
      ],
      "metadata": {
        "id": "iwRkC-cDgZKx"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(tfidf, y, test_size=0.2, stratify=y, random_state=1234)\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=1234, verbose=1, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "H-R30726gZH4"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tfidf.info()"
      ],
      "metadata": {
        "id": "Y-CZQslXgY-v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import gc\n",
        "del(bow)\n",
        "del(tfidf)\n",
        "gc.collect()\n",
        "bow=pd.DataFrame()\n",
        "tfidf=pd.DataFrame()"
      ],
      "metadata": {
        "id": "IjmKHRv9meuY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Text preprocessing \n",
        "* Removing punctuations\n",
        "* Removing Stopwords\n",
        "* Lowercase\n",
        "* Tokenization\n",
        "* Stemming / Lemmatization"
      ],
      "metadata": {
        "id": "zbKzAUuQoFoz"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X = data_selected['내원동기']\n",
        "X.head()"
      ],
      "metadata": {
        "id": "PTfYPMRPoCB3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.iloc[1]"
      ],
      "metadata": {
        "id": "VQ5v8bQCqCTX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Tokenization\n",
        "\n",
        "### 한국어 자연어 처리를 위해서는 KoNLPy(코엔엘파이)라는 파이썬 패키지를 사용할 수 있습니다. 코엔엘파이를 통해서 사용할 수 있는 형태소 분석기로 Okt(Open Korea Text), 메캅(Mecab), 코모란(Komoran), 한나눔(Hannanum), 꼬꼬마(Kkma)가 있습니다.\n",
        "\n"
      ],
      "metadata": {
        "id": "WNDSGCwbsDnT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from konlpy.tag import Okt, Kkma, Komoran\n",
        "\n",
        "okt = Okt()\n",
        "kkma = Kkma()\n",
        "komoran = Komoran()"
      ],
      "metadata": {
        "id": "WF0I-uhT-oqw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('OKT 형태소 분석 :',kkma.morphs(X.iloc[1]))\n",
        "print('OKT 품사 태깅 :',kkma.pos(X.iloc[1]))\n",
        "print('OKT 명사 추출 :',kkma.nouns(X.iloc[1])) "
      ],
      "metadata": {
        "id": "IrTaEmcyr99o"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('OKT 형태소 분석 :',komoran.morphs(X.iloc[1]))\n",
        "print('OKT 품사 태깅 :',komoran.pos(X.iloc[1]))\n",
        "print('OKT 명사 추출 :',komoran.nouns(X.iloc[1])) "
      ],
      "metadata": {
        "id": "H1H1HMTCr97J"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('OKT 형태소 분석 :',okt.morphs(X.iloc[1]))\n",
        "print('OKT 형태소 분석 :',kkma.morphs(X.iloc[1]))\n",
        "print('OKT 형태소 분석 :',komoran.morphs(X.iloc[1]))\n"
      ],
      "metadata": {
        "id": "Ts7ZLNwIwAC6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "1UwHAdWrwAAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## soynlp\n",
        "\n",
        "#### soynlp 에서 제공하는 WordExtractor 나 NounExtractor 는 여러 개의 문서로부터 학습한 통계 정보를 이용하여 작동합니다. 비지도학습 기반 접근법들은 통계적 패턴을 이용하여 단어를 추출하기 때문에 하나의 문장 혹은 문서에서 보다는 어느 정도 규모가 있는 동일한 집단의 문서 (homogeneous documents) 에서 잘 작동합니다. 영화 댓글들이나 하루의 뉴스 기사처럼 같은 단어를 이용하는 집합의 문서만 모아서 Extractors 를 학습하시면 좋습니다. 이질적인 집단의 문서들은 하나로 모아 학습하면 단어가 잘 추출되지 않습니다."
      ],
      "metadata": {
        "id": "viv0VmdPxNHn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ListCorpus:\n",
        "    def __init__(self, doc_list):\n",
        "        self.corpus = doc_list\n",
        "    def __len__(self):\n",
        "        return len(self.corpus)\n",
        "    def __iter__(self):\n",
        "        for doc in self.corpus:\n",
        "            yield doc\n",
        "\n",
        "## WordExtractor --> L-Tokenizer\n",
        "doc_iter = ListCorpus(X.to_list())\n",
        "word_extractor = WordExtractor(min_frequency=10, min_cohesion_forward=0.1, min_right_branching_entropy=0.0)\n",
        "word_extractor.train(doc_iter)\n",
        "word_scores = word_extractor.extract()\n",
        "cohesion_scores = {word:score.cohesion_forward for word, score in word_scores.items()}\n",
        "ltokenizer = LTokenizer(scores=cohesion_scores)"
      ],
      "metadata": {
        "id": "ydKOjY1-wVbh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(ltokenizer.tokenize(X.iloc[1]))"
      ],
      "metadata": {
        "id": "3mKIMs8IwVZD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Lowercase, punctuation, stopwords "
      ],
      "metadata": {
        "id": "6HsgvmrO1YPF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocessing(text):\n",
        "    # 개행문자 제거\n",
        "    text = re.sub('\\\\\\\\n', ' ', text)\n",
        "    \n",
        "    # 특수문자 제거\n",
        "    text = re.sub('[?.,;:|\\)*~`’!^\\-_+<>@\\#$%&-=#}※]', ' ', text)\n",
        "    \n",
        "    # 한글, 영문만 남기고 모두 제거\n",
        "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', text)\n",
        "\n",
        "    # lowercase \n",
        "    text = text.lower()\n",
        "    \n",
        "    return text\n",
        "\n",
        "X = data_selected['내원동기'].apply(preprocessing)"
      ],
      "metadata": {
        "id": "aPCzh_chwVRD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "zOVwspfCwVOS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.iloc[1]"
      ],
      "metadata": {
        "id": "Rrvi48i_qCQw"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(X.iloc[1]))"
      ],
      "metadata": {
        "id": "7iLoM84GqCLm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(okt.morphs(X.iloc[1], stem=True))"
      ],
      "metadata": {
        "id": "12_JJ4qx2lVv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "stopwords = ['가', '가령', '각', '각각', '각자', '각종', '같다', '같이', '거바', '거의', '것', '겨우', '고로', '곧', '과', '과연', '관한', \n",
        "             '구', '그', '그들', '그때', '그래', '그럼', '그저', '기타', '까악', '까지', '꽈당', '끙끙', '끼익', '나', '남들', '남짓', '너', \n",
        "             '너희', '네', '넷', '년', '다른', '다섯', '다소', '다수', '다음', '단지', '답다', '당신', '당장', '댕그', '동안', '둘', '둥둥', \n",
        "             '들', '등', '등등', '딩동', '따라', '따위', '때', '또', '또한', '뚝뚝', '라령', '로', '로써', '륙', '를', '마저', '마치', '만큼', \n",
        "             '매번', '몇', '모', '모두', '무렵', '무슨', '무엇', '물론', '및', '버금', '봐', '봐라', '붕붕', '비록', '삐걱', '사', '삼', '설령', \n",
        "             '설마', '설사', '셋', '소생', '소인', '솨', '쉿', '시각', '시간', '실로', '아', '아니', '아야', '아이', '아하', '아홉', '앗', '야', \n",
        "             '약간', '양자', '어', '어느', '어디', '어때', '어떤', '어이', '언제', '얼마', '엉엉', '에', '에서', '여', '여기', '여덟', '여부', \n",
        "             '여섯', '여차', '영', '영차', '예', '오', '오직', '오호', '와', '와아', '왜', '우리', '우선', '월', '윙윙', '육', '으로', '을', '응', \n",
        "             '응당', '의', '의해', '이', '이곳', '이때', '이래', '이런', '이번', '이봐', '이상', '이젠', '이쪽', '일', '일곱', '일단', '일때', \n",
        "             '있다', '자', '자기', '자신', '잠깐', '잠시', '저', '저것', '저기', '저쪽', '저희', '전부', '전자', '전후', '조금', '조차', '졸졸', \n",
        "             '좀', '좋아', '좍좍', '즉', '즉시', '지만', '참', '참나', '쳇', '콸콸', '쾅쾅', '쿵', '타다', '타인', '탕탕', '툭', '퉤', '틈타', '팍', \n",
        "             '팔', '퍽', '펄렁', '하', '하나', '하하', '할뿐', '함께', '해요', '허', '허걱', '허허', '헉', '헉헉', '혹은', '혼자', '훨씬', '휘익', \n",
        "             '휴', '흐흐', '흥']"
      ],
      "metadata": {
        "id": "elMsepCW2lPy"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def processing(text):\n",
        "    words = okt.morphs(text, stem=True)\n",
        "    text = [word for word in words if not word in stopwords]\n",
        "    text = ' '.join(text)\n",
        "    return text    \n",
        "\n",
        "X = X.apply(processing)"
      ],
      "metadata": {
        "id": "x72YuaoD2lNJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X.head()"
      ],
      "metadata": {
        "id": "8S00C-vK2lK0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countvec = CountVectorizer(lowercase=False)\n",
        "cdf = countvec.fit_transform(X)\n",
        "bow = pd.DataFrame(cdf.toarray(), columns = countvec.get_feature_names())\n",
        "bow\n"
      ],
      "metadata": {
        "id": "rOQtTm3M2lIL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(bow, y, test_size=0.2, stratify=y, random_state=1234)\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=1234, verbose=1, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "ShQtN5IZ2lFL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# simple text processing \n",
        "replace_words = {'\\n' : ' ', ',' : ' ', '(+)' : ' positive ', '(+/+)' : ' positive positive', '(-)' : ' negative ', \n",
        "                 '(-/-)' : ' negative negative ', '(-/-/-)' : ' negative negative negative ', '+/+' : ' positive positive ', \n",
        "                 '+/-' : ' positive negative ', '-/+' : ' negative positive ', '-/-' : ' negative negative ', 'lt' : ' left ', \n",
        "                 'rt' : ' right ', 'c/s' : ' cough sputum ', 'c/s/r' : ' cough sputum rhinorrhea ', \n",
        "                 'n/v/d' : ' nausea vomiting diarrhea ', 'g/o' : ' gas out ', 'g/w' : ' general weakness ', \n",
        "                 'n/v' : ' nausea vomiting ', '/day' : ' per day ', 'b/cr' : ' bun cr', 'b/c' : ' bun cr', 'a-ct' : ' abdominal ct ', \n",
        "                 'f/e' : ' further evaluation ', 'l/e' : ' lower extremity ', 'u/e' : ' upper extremity ', 'u/o' : ' urine output ', \n",
        "                 'v/s' : ' vital sign ', 'w/u' : ' work up ', 'd/t' : ' due to ', 'f/u' : ' follow up ', 'f/e' : ' further evaluation ', \n",
        "                 'g/w' : ' general weakness ', 'n/v' : ' nausea vomiting ', 'r/o' : ' rule out ', 's/o' : ' stitch out ', \n",
        "                 's/p' : ' status post ', 'u/o' : ' urine output ', 'v/s' : ' vital sign ', 'w/u' : ' work up ', 'da' : ' days ago ', \n",
        "                 'wa' : ' weeks ago ', 'ha' : ' headache ', 'abd' : ' abdomen ', 'ha' : ' headache ', 'rt' : ' right ', 'lt' : ' left ', \n",
        "                 '+' : ' positive ', 'chemo' : ' chemotherapy ', 'ctx' : ' chemotherapy ', 'op' : ' operation ', 'r/o' : ' rule out ', \n",
        "                 'dz' : ' dizziness ', 'fx' : ' fracture ', 'hb' : ' hemoglobin ', 'spo2' : ' saturation ', 'sx' : ' symptom ', \n",
        "                 'mg/dl' : ' milligram per deciliter ', '2nd' : ' second ', 'ta' : ' traffic accident ', 'ctx:' : ' chemotherapy ', \n",
        "                 'preg' : ' pregnancy ', 'hr' : ' hour ', 'ca' : ' cancer ', 'n&v' : ' nausea vomiting ', 'iv' : ' intravenous ', 'pn' : \n",
        "                 ' pneumonia ', 'aki' : ' acute kidney injury ', 'bts' : ' blood tinged sputum ', '5th' : ' fifth ', \n",
        "                 'chemo:' : ' chemotherapy ', 'lft' : ' liver function test ', '4th' : ' fourth ', 'sz' : ' seizure ', 'ekg' : ' ecg ', \n",
        "                 'gtc' : ' general tonic clonic ', 'b-ct' : ' brain ct ', 'tb' : ' tuberculosis ', 'lnt' : ' last normal time ', \n",
        "                 'g' : ' gram ', 'sdh' : ' subdural hemorrhage ', 'ua' : ' urinalysis ', 'dm' : ' diabetes mellitus ', \n",
        "                 '0a' : ' midnight ', '1a' : ' one am ', '2a' : ' two am ', '3a' : ' three am ', '4a' : ' four am ', '5a' : ' five am ', \n",
        "                 '6a' : ' six am ', '7a' : ' seven am ', '8a' : ' eight am ', '9a' : ' nine am ', '10a' : ' ten am ', \n",
        "                 '11a' : ' eleven am ', '1p' : ' one pm ', '2p' : ' two pm ', '3p' : ' three pm ', '4p' : ' four pm ', \n",
        "                 '5p' : ' five pm ', '6p' : ' six pm ', '7p' : ' seven pm ', '8p' : ' eight pm ', '9p' : ' nine pm ', \n",
        "                 '10p' : ' ten pm ', '11p' : ' eleven pm ', '12p' : ' midday ', 'k' : ' potassium ', 'bx' : ' biopsy ', \n",
        "                 '1st' : ' first ', 'gi' : ' gastrointestinal ', 'd50w' : ' dextrose ', 'cxr' : ' chest xray ', 'anti' : ' antibiotics ', \n",
        "                 'ama' : ' against medical advice ', 'ich' : ' intracranial hemorrhage ', 'pf' : ' professor ', 'mx' : ' management ', \n",
        "                 'b-mri' : ' brain mri ', 'n-fever' : ' neutropenic fever ', '2th' : ' second ', 'htn' : ' hypertension ', \n",
        "                 'loc' : ' loss of consciousness ', 'sl' : ' slight ', 'c-line' : ' central line ', 'hcc' : ' hepatocellular carcinoma ', \n",
        "                 'mi' : ' myocardial infarct ', 'p/t' : ' pain tenderness ', 'epi' : ' epigastric ', 'ot' : ' 안과 ', 'ant' : ' anterior ', \n",
        "                 't bil' : ' total bilirubin ', 'f/u' : ' follow up ', 'sz' : ' seizure ', 'pul' : ' pulmonary ', 'wfr' : ' warfarin ', \n",
        "                 'ckd' : ' chronic kidney disease ', 'a.fib' : ' atrial fibrillation ', 'i&d' : ' incision and drainage ', \n",
        "                 'painnrs' : ' pain nrs ', 'ot/pt' : ' ast alt ', 'poi' : ' poor oral intake ', 'ta' : ' traffic accident ', \n",
        "                 'n/s' : ' normal saline ', 'nsr' : ' normal sinus rhythm ', 'uti' : ' urinary tract infection ', 'pn' : ' pneumonia ', \n",
        "                 'po' : ' oral ', 'b-mr' : ' brain mri ', 'x-ray' : ' xray ', 'g' : ' gram ', 'em' : ' 응급의학과 ', \n",
        "                 'im' : ' 내과 ', 'gs' : ' 외과 ', 'nr' : ' 신경과 ', 'np' : ' 정신과 ', 'fm' : ' 가정의학과 ', 'ent' : ' 이비인후과 ', \n",
        "                 'obgy' : ' 산부인과 ', 'shuolder' : ' shoulder ', 'p/e' : ' physical exam ', 'loc-' : ' no loc ', 'cxr' : ' chest xray ', \n",
        "                 'cm' : ' centimeter ', 'tf' : ' transfer ', 'ts' : ' 흉부외과 '}\n",
        "\n",
        "\n",
        "def preprocessing(text):\n",
        "\n",
        "    def splitWord(str):\n",
        "        regex2 = r\"[\\u4e00-\\ufaff]+|[0-9a-zA-Z\\/\\&\\-\\+\\(\\)]+|\\,\"\n",
        "        matches = re.findall(regex2, str, re.UNICODE)\n",
        "        return matches\n",
        "\n",
        "    text = text.replace('(', ' (')\n",
        "    text = text.replace(')', ') ')\n",
        "    text = ' '.join([replace_words.get(n.strip(), n) for n in splitWord(text)])\n",
        "    text = re.sub('[?.,;:|\\)*~`’!^\\-_+<>@\\#$%&-=#}※]', ' ', text)\n",
        "    text = re.sub('[^가-힣ㄱ-ㅎㅏ-ㅣa-zA-Z]', ' ', text)\n",
        "    text = text.lower()\n",
        "    words = okt.morphs(text, stem=True)\n",
        "    text = [word for word in words if not word in stopwords]\n",
        "    text = ' '.join(text)\n",
        "    \n",
        "    return text\n",
        "\n",
        "X = data_selected['내원동기'].apply(preprocessing)"
      ],
      "metadata": {
        "id": "d7EOFd8vqCI9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "countvec = CountVectorizer(lowercase=False)\n",
        "cdf = countvec.fit_transform(X)\n",
        "bow = pd.DataFrame(cdf.toarray(), columns = countvec.get_feature_names())\n",
        "X_train, X_test, y_train, y_test = train_test_split(bow, y, test_size=0.2, stratify=y, random_state=1234)\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=1234, verbose=1, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "u_wBp5kb9uCT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Both data types"
      ],
      "metadata": {
        "id": "k0u61oRGBKbd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "X_c = data_selected.drop(['KTAS level', '내원동기'], axis=1).reset_index(drop=True)\n"
      ],
      "metadata": {
        "id": "y02YXled9t9R"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_c.head()"
      ],
      "metadata": {
        "id": "dcnM2goo9t61"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "bow.head()"
      ],
      "metadata": {
        "id": "fDqF6jg0_sne"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both = pd.merge(bow, X_c, left_index=True, right_index=True)"
      ],
      "metadata": {
        "id": "0VKLtzde_sk_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "both.head()"
      ],
      "metadata": {
        "id": "A-FWofnj_siZ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X_train, X_test, y_train, y_test = train_test_split(both, y, test_size=0.2, stratify=y, random_state=1234)\n",
        "rf = RandomForestClassifier(n_estimators=50, random_state=1234, verbose=1, n_jobs=-1)\n",
        "rf.fit(X_train, y_train)\n",
        "y_pred = rf.predict(X_test)\n",
        "print(accuracy_score(y_test, y_pred))\n",
        "print(confusion_matrix(y_test, y_pred))\n",
        "print(classification_report(y_test, y_pred))"
      ],
      "metadata": {
        "id": "jlmzFlBu_sf0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "8F5yEdS6_sdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}